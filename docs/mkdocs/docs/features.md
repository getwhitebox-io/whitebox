# Features

## Design decisions

- **Easy**: Very easy to set up and get started with.
- **Intuitive**: Designed to be intuitive and easy to use.
- **Pythonic SDK**: Pythonic SDK for building your own monitoring infrastructure.
- **Robust**: Get production-ready MLOps system.
- **Kubernetes**: Get production-ready code. With automatic interactive documentation.

## Descriptive Statistics

Whitebox provides a nice [list of descriptive statistics](../metric-definitions/#descriptive-statistics) of input dataset, making the overview of data easy.

## Classification Models Metrics

Whitebox includes comprehensive [metrics](../metric-definitions/#evaluation-metrics) tracking for classification models. This allows users to easily evaluate the performance of their classification models and identify areas for improvement. Additionally, users can set custom thresholds for each metric to receive alerts when performance deviates from expected results.

## Data / Concept Drift Monitoring

Whitebox includes monitoring for data and concept drift. This feature tracks changes in the distribution of the data used to train models and alerts users when significant changes occur. Additionally, it detects changes in the performance of deployed models and alerts users when significant drift is detected. This allows users to identify and address data and model drift early, reducing the risk of poor model performance.

## Explainable AI

Whitebox includes model explaination also. The explainability performed through the explainability report which allows user to know anytime which feature had the most impact on model's prediction.

## Alerts

Whitebox includes an alerting system that allows users to set custom thresholds for metrics and receive notifications when performance deviates from expected results. These alerts can be delivered via email, SMS, or webhook, and can be customized to fit the needs of any organization. This allows users to quickly respond to changes in model performance and take action to improve results.
